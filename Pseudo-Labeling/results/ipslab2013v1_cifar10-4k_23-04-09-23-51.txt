['/content/drive/.shortcut-targets-by-id/1rq7ZwO6n9Qq8z-u9Ls5ROqESUr9unPFF/AdversarialAI/Pseudo-Labeling', '/content/drive/MyDrive/AdversarialAI/Poisoner', '/env/python', '/usr/lib/python39.zip', '/usr/lib/python3.9', '/usr/lib/python3.9/lib-dynload', '/usr/local/lib/python3.9/dist-packages', '/usr/lib/python3/dist-packages', './']
Namespace(print_freq=20, save_freq=100, save_dir='./checkpoints', dataset='cifar10', workers=4, num_labels=4000, sup_batch_size=64, usp_batch_size=64, data_twice=False, data_idxs=False, label_exclude=None, arch='cnn13', model='ipslab2013v1', drop_ratio=0.0, epochs=400, optim='sgd', momentum=0.9, nesterov=True, weight_decay=0.0005, lr=0.1, lr_scheduler='cos', min_lr=0.0001, steps=None, gamma=None, rampup_length=80, rampdown_length=50, t1=None, t2=None, soft=None, xi=None, eps=None, n_power=None, threshold=None, ema_decay=None, mixup_alpha=None, usp_weight=1.0, weight_rampup=30, ent_weight=None)
pytorch version : 2.0.0+cu118
getting cifar10 | subset size, labeled size, test size = [4500, 400, 1000]
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
labeled, unlabeled, test sizes: [400, 4100, 1000]
malicious: airplane , target: automobile
x_star: <class 'numpy.ndarray'>
x_target: <class 'numpy.ndarray'>
target and malicious images saved to ./data/poison/latest_poison
label_idxs: [1982, 1855, 4425, 347, 2871, 3463, 4039, 151, 3935, 4487, 1911, 3548, 1262, 437, 1081, 3947, 2152, 1411, 4391, 560, 648, 80, 1706, 2051, 3284, 2954, 1172, 1593, 4357, 4469, 1099, 211, 4465, 1218, 1198, 2388, 1724, 2032, 2612, 1679, 1971, 3902, 4377, 3526, 4485, 4344, 47, 2249, 1814, 1888, 3661, 622, 3211, 946, 3779, 986, 3770, 3272, 925, 2083, 1957, 806, 4284, 2232, 2877, 4262, 4288, 3726, 75, 1056, 1432, 2998, 943, 542, 2332, 2166, 1571, 2912, 1399, 2606, 38, 1148, 4057, 3778, 4162, 1733, 2973, 2685, 1959, 345, 2485, 3327, 3246, 3712, 3224, 503, 3884, 1660, 2667, 1770, 1985, 1669, 1082, 2195, 2203, 3145, 1678, 2208, 4476, 3078, 3408, 4436, 1819, 1550, 1065, 3597, 3028, 708, 3939, 4378, 2334, 969, 1787, 504, 2487, 1893, 92, 2450, 223, 3492, 1968, 59, 500, 3448, 3361, 401, 2697, 1364, 756, 2627, 427, 1723, 2784, 4036, 1677, 3069, 4274, 1021, 1009, 224, 3992, 1805, 4307, 3452, 3662, 4333, 768, 282, 3550, 4319, 2907, 3925, 4047, 3876, 1828, 4421, 327, 1848, 2586, 2567, 1545, 860, 2836, 2530, 2008, 3446, 283, 2710, 2738, 1908, 3290, 830, 2448, 4439, 3064, 2108, 967, 279, 2048, 2497, 3781, 2726, 895, 291, 1638, 1152, 2313, 403, 2604, 1569, 1697, 417, 2177, 3456, 3264, 989, 4498, 2348, 418, 2890, 86, 4224, 2961, 4033, 3506, 3299, 2415, 2153, 2957, 4460, 4414, 3991, 837, 2059, 99, 1320, 1253, 2180, 3889, 787, 2975, 4464, 1801, 1180, 150, 2727, 820, 4271, 3360, 1981, 884, 522, 3362, 2524, 1912, 1176, 2619, 977, 3293, 1513, 4081, 2657, 1582, 764, 4388, 861, 740, 2158, 2753, 207, 364, 4474, 3239, 3258, 4183, 1283, 2526, 2331, 2499, 1392, 3867, 2338, 3484, 1015, 1368, 3534, 1886, 3658, 3982, 970, 2891, 846, 250, 3777, 4399, 728, 2559, 3566, 2665, 1079, 4069, 619, 2215, 4243, 3192, 1534, 4167, 1637, 690, 4310, 2060, 275, 2408, 3443, 4373, 3018, 1257, 302, 241, 650, 2085, 1451, 3494, 3574, 1892, 3756, 2874, 1728, 1874, 1431, 4246, 2363, 3244, 778, 1927, 2360, 3842, 2532, 4292, 1429, 3575, 1317, 949, 3540, 3062, 1373, 265, 1316, 2833, 3949, 2875, 4180, 2267, 5, 4007, 593, 2935, 4364, 18, 4230, 409, 2691, 3706, 2918, 4402, 301, 495, 3061, 2948, 4315, 2882, 4253, 1255, 2522, 1906, 1920, 3549, 3347, 1418, 4107, 2701, 2786, 548, 950, 1291, 2523, 300, 1488, 3294, 133, 4293, 1440, 598, 3286, 3434, 4428, 1197, 2970, 1289, 3804, 3521, 3252, 1495, 2159, 1038, 1295, 3557, 4477, 3435, 3846, 120, 2751, 1029, 4339, 817, 3618, 2779, 3367, 4150, 4419, 3500, 2343, 919, 1719, 4147, 1165, 3993, 2187, 2720, 2211, 4455, 2854, 1714, 2018, 2824, 2381, 1141, 2164, 3774, 2282, 1467, 3542, 171, 3161, 2509, 2552, 3682, 2057, 2430, 727, 3596, 1639, 2723, 1503, 1286, 1938, 545, 614, 2793, 2004, 930, 4098, 754, 4105, 4189, 1496, 3301, 4044, 3428, 4447, 4195, 4071, 3187, 4232, 1167, 2149, 2146, 4051, 1089, 1852, 1447, 852, 1492, 2303, 809, 2680, 1907, 1765, 824, 1139, 3922, 565, 1491, 2585, 2886, 566, 4141, 3808, 4231, 4303, 2743, 3531, 2712, 790, 2452, 3228, 308, 2464, 3613, 431, 2308, 3834, 62, 3987, 2330, 544, 3230, 2758, 3943, 2088, 1931, 2880, 1481, 193, 2483, 1755, 2168, 3760, 159, 635, 1022, 1954, 217, 2926, 3136, 204, 3696, 2294, 4358, 419, 3831, 1433, 4070, 2131, 3546, 2623, 2593, 172, 2498, 4091, 2689, 3608, 2775, 21, 556, 294, 2983, 1097, 643, 148, 1232, 3673, 2843, 854, 2493, 3146, 4037, 3185, 3782, 4169, 4088, 4210, 3266, 3454, 2025, 1067, 3169, 3480, 2767, 3637, 4431, 4350, 2814, 3354, 4166, 2692, 2142, 3898, 2798, 3771, 117, 1179, 1745, 1408, 1554, 176, 3893, 2002, 3084, 1556, 3718, 281, 3183, 1738, 3432, 903, 2502, 3920, 3411, 3376, 3474, 3308, 538, 1478, 3568, 964, 1865, 420, 3243, 4264, 2320, 3340, 1260, 1076, 3951, 74, 874, 94, 3442, 3409, 2066, 2293, 1412, 2479, 968, 51, 3582, 386, 2436, 921, 4306, 3250, 3880, 2986, 4192, 4115, 1033, 3177, 3410, 3503, 3109, 2540, 3822, 2832, 1576, 1448, 29, 3466, 3201, 537, 388, 2189, 2120, 1450, 2819, 333, 657, 2419, 1843, 2417, 2092, 4362, 2326, 2903, 752, 1751, 4123, 3345, 1553, 261, 2707, 952, 3963, 2096, 3979, 2462, 1919, 2141, 1658, 4435, 2199, 1248, 561, 2270, 290, 524, 1634, 234, 3124, 2574, 815, 2956, 691, 942, 2773, 4188, 1612, 4005, 343, 2046, 550, 122, 867, 3143, 4278, 1820, 1023, 3364, 69, 2866, 1598, 1500, 3440, 3610, 3533, 2605, 2853, 988, 1005, 1519, 2590, 4352, 2543, 1878, 3586, 3200, 1457, 2763, 330, 2349, 430, 3085, 3946, 1967, 4268, 1145, 1304, 2533, 212, 1826, 2016, 2339, 784, 3745, 4003, 3213, 4190, 3699, 3983, 3381, 2931, 3977, 2459, 3680, 2979, 452, 1707, 130, 954, 1375, 2718, 2185, 4392, 484, 701, 2641, 4429, 1204, 1900, 1196, 850, 4215, 2576, 1621, 940, 2290, 335, 3488, 2028, 3276, 1778, 555, 128, 4313, 1910, 2079, 1217, 1505, 4062, 2976, 2617, 3423, 2126, 3171, 2145, 4198, 2896, 814, 105, 4001, 3188, 3741, 715, 937, 479, 3292, 4304, 2642, 2637, 2255, 3343, 3530, 289, 2247, 4312, 661, 3697, 1773, 1890, 1300, 898, 3025, 4418, 603, 682, 3077, 659, 3775, 1381, 1386, 2364, 2934, 1234, 3558, 2852, 3906, 1763, 1319, 1646, 600, 4371, 1256, 4068, 4100, 3034, 2917, 1230, 1073, 3455, 2547, 1349, 579, 4368, 1144, 1184, 1443, 3640, 4163, 1567, 119, 1972, 3670, 3072, 1791, 48, 4488, 3126, 2550, 3764, 2251, 2418, 4280, 198, 927, 2045, 3426, 762, 681, 4342, 554, 2951, 3828, 4148, 1236, 259, 725, 4157, 162, 2026, 1993, 3765, 173, 4318, 2828, 4022, 2683, 1351, 1127, 3365, 2529, 2410, 1813, 2695, 4286, 2690, 3907, 3141, 3265, 807, 3628, 4060, 3173, 3370, 1336, 3202, 2465, 1004, 913, 959, 2001, 1837, 2340, 2869, 733, 1224, 2161, 3184, 597, 3899, 1380, 4341, 81, 491, 1109, 1715, 1932, 1276, 1856, 2147, 4305, 1474, 118, 641, 3321, 483, 30, 1363, 3510, 782, 1777, 2624, 894, 1605, 829, 1325, 184, 3475, 4184, 2105, 2610, 3095, 2379, 328, 3189, 1396, 1390, 4241, 2021, 2888, 1378, 3859, 85, 4452, 1314, 4138, 951, 1036, 4031, 3547, 2367, 2511, 2842, 1460, 801, 1181, 2616, 3864, 1610, 3685, 3233, 323, 835, 33, 3058, 2023, 1406, 3181, 1192, 3159, 1130, 608, 3677, 901, 2909, 2725, 4479, 1379, 1001, 3469, 2958, 3743, 4127, 4213, 2577, 3305, 658, 2747, 4366, 1404, 827, 1352, 4131, 4482, 4030, 3849, 2260, 1493, 1997, 4347, 3518, 1898, 2144, 3398, 3070, 1499, 896, 2491, 2942, 1020, 91, 3973, 2307, 177, 1705, 4426, 4481, 244, 3468, 4073, 574, 1462, 2862, 1138, 533, 4077, 278, 3588, 2296, 3237, 1538, 1284, 1743, 1205, 2097, 3971, 514, 2407, 1162, 1754, 955, 310, 2440, 4133, 2404, 2344, 4265, 2416, 1240, 4191, 2476, 2474, 1461, 1671, 1655, 1999, 1466, 434, 268, 1037, 823, 3753, 1833, 182, 3071, 4311, 221, 2329, 906, 1987, 1287, 584, 714, 4390, 1764, 3723, 4456, 27, 3020, 3470, 2959, 590, 1528, 3366, 380, 1277, 689, 976, 1588, 1539, 462, 189, 3282, 1832, 1132, 114, 2373, 1264, 1623, 637, 3127, 1827, 3728, 3094, 3603, 266, 2977, 1604, 363, 3942, 156, 3940, 2082, 508, 1953, 3655, 2328, 2275, 227, 497, 592, 1933, 1437, 3102, 358, 1849, 1871, 4099, 2766, 215, 2345, 116, 3602, 3005, 1565, 4322, 842, 4336, 4093, 3516, 2563, 4432, 1341, 2020, 4170, 2737, 3981, 1806, 1111, 2439, 1735, 152, 2184, 4433, 2964, 944, 3344, 2013, 1799, 2666, 2163, 3405, 2106, 321, 3232, 2700, 4367, 649, 4359, 2859, 304, 4074, 3806, 273, 2750, 2071, 4248, 853, 4422, 3620, 435, 1131, 1310, 4285, 2123, 1989, 2734, 1540, 1564, 2217, 2216, 1066, 1376, 941, 2840, 2704, 127, 2399, 2674, 4445, 3097, 3532, 3269, 1699, 2239, 1673, 771, 1949, 2089, 1635, 3724, 2389, 2039, 776, 1631, 3003, 3279, 1978, 3791, 1449, 848, 1737, 1055, 3818, 2122, 1393, 450, 4326, 775, 1929, 2209, 1146, 2230, 4495, 1362, 4179, 647, 881, 2838, 1836, 3338, 389, 4451, 4244, 3331, 2910, 1941, 313, 3316, 1876, 2504, 984, 2310, 2878, 4355, 7, 2261, 3216, 4114, 3821, 4237, 1302, 1013, 493, 3810, 2455, 1775, 2989, 472, 1027, 3643, 1282, 3813, 4000, 912, 2359, 1222, 564, 3941, 89, 832, 3839, 4346, 2263, 3317, 3386, 3151, 869, 818, 907, 1387, 2651, 958, 1636, 1670, 3300, 138, 4468, 3904, 2077, 1091, 379, 2183, 800, 1504, 387, 3502, 4316, 2615, 3601, 4225, 1632, 855, 724, 4137, 3229, 485, 65, 2927, 1303, 2118, 821, 996, 3634, 3590, 3769, 3676, 2962, 3585, 168, 3429, 1838, 429, 3594, 4354, 3262, 3014, 4331, 1783, 3467, 4197, 3495, 2160, 3646, 1894, 453, 1407, 3573, 505, 1518, 3929, 4386, 1250, 705, 3013, 3653, 3009, 359, 692, 4012, 3296, 4254, 97, 3868, 3874, 1703, 2579, 499, 1847, 2027, 2463, 3193, 2870, 2594, 3218, 190, 515, 3144, 2825, 1098, 2309, 341, 3598, 3214, 698, 2702, 200, 1053, 2346, 331, 3519, 3830, 510, 2054, 1844, 2851, 2099, 1140, 865, 730, 3273, 4489, 3368, 1075, 3543, 3663, 3191, 1753, 1083, 2100, 3704, 693, 2258, 3664, 3022, 3612, 819, 2347, 144, 2252, 857, 1311, 2391, 50, 1885, 2545, 3976, 1543, 1400, 1903, 2978, 3659, 3489, 1012, 863, 1389, 1507, 2744, 1007, 1242, 406, 4034, 1974, 2736, 868, 3147, 3000, 777, 2093, 2116, 2603, 1288, 710, 3215, 3824, 422, 1263, 1135, 2991, 3872, 1589, 791, 1267, 218, 502, 3130, 1416, 1851, 4375, 4082, 3068, 2730, 4471, 1517, 3671, 886, 1279, 4135, 267, 3160, 4079, 1795, 679, 1, 1664, 463, 956, 3016, 2278, 3763, 352, 488, 4269, 3460, 4467, 471, 3401, 3936, 1921, 3827, 4200, 2864, 847, 3693, 3924, 1712, 40, 3416, 552, 1102, 1629, 746, 726, 3385, 88, 4329, 3998, 4413, 1951, 2218, 2207, 3121, 4461, 2318, 1683, 2647, 4290, 644, 239, 2640, 1185, 1625, 4168, 2162, 2306, 3413, 3178, 1403, 596, 2671, 2560, 1992, 957, 2655, 572, 3978, 425, 1195, 1687, 2705, 3042, 263, 3966, 3336, 1530, 2649, 3901, 1746, 4075, 2898, 621, 2709, 3065, 3393, 4379, 1798, 3281, 880, 3303, 1178, 4301, 3458, 399, 1445, 2873, 3108, 2847, 2337, 3117, 663, 918, 859, 3024, 1690, 741, 2090, 2990, 3700, 4376, 1268, 309, 55, 423, 3702, 934, 3001, 4050, 3153, 1237, 2820, 1522, 3123, 1459, 1502, 1017, 3509, 1768, 2794, 739, 770, 1358, 3253, 1014, 1835, 3857, 2790, 2944, 1750, 398, 4256, 3583, 3789, 1983, 254, 1990, 1794, 2755, 1110, 687, 2578, 4405, 226, 4223, 2110, 892, 2548, 4229, 1446, 3012, 3710, 1615, 3850, 14, 110, 3227, 655, 2698, 2549, 2693, 1331, 2620, 1547, 2974, 4041, 1052, 962, 2383, 2922, 2236, 1758, 620, 2571, 948, 1618, 3444, 1227, 686, 84, 2764, 3796, 1541, 3098, 4164, 6, 2686, 3749, 974, 1301, 4023, 1046, 3478, 2783, 4420, 965, 1142, 1520, 2133, 3247, 3481, 871, 1428, 3155, 76, 2204, 4158, 3869, 1561, 1698, 713, 3732, 4353, 362, 4258, 470, 4381, 3099, 1202, 651, 536, 3352, 1244, 2314, 2356, 2245, 617, 611, 1247, 3985, 2884, 1010, 4063, 1231, 1212, 2731, 305, 1666, 849, 2721, 4181, 1120, 4140, 2124, 3829, 1757, 3910, 77, 576, 3709, 408, 4250, 4308, 1663, 2813, 3041, 616, 2225, 3552, 4045, 2996, 1861, 3140, 1194, 4142, 1896, 2192, 4251, 2945, 1684, 1019, 816, 3439, 3328, 1114, 32, 9, 3967, 115, 4497, 2333, 20, 2151, 3589, 2231, 2280, 3031, 4216, 2809, 2394, 1483, 3933, 1119, 1822, 3254, 2254, 2456, 4334, 4361, 1710, 2745, 774, 3036, 213, 3208, 3545, 3635, 3306, 2807, 3816, 2658, 1976, 2867, 1269, 2855, 2257, 793, 8, 185, 3377, 1246, 3148, 382, 2670, 2069, 325, 1365, 2893, 4411, 4484, 101, 1681, 671, 758, 668, 2643, 2000, 1531, 900, 329, 3678, 3639, 3799, 2193, 3965, 1662, 4396, 2555, 3812, 4149, 1630, 360, 1072, 2486, 1741, 1627, 1866, 3205, 197, 1274, 4136, 1696, 2815, 3342, 3047, 53, 377, 673, 2119, 916, 2538, 1126, 3180, 316, 990, 3337, 573, 2400, 2781, 3044, 720, 1497, 797, 4448, 199, 1116, 3060, 1101, 3485, 2350, 1173, 1154, 473, 231, 2237, 2518, 1879, 2928, 3015, 2325, 3389, 1243, 4287, 1521, 1570, 1661, 1208, 1063, 3051, 1383, 3023, 3046, 1928, 2104, 4172, 4080, 1068, 280, 772, 4343, 2633, 2102, 866, 2375, 3980, 872, 2056, 1579, 581, 3705, 2174, 992, 534, 468, 2632, 3780, 2588, 1338, 2445, 2369, 3418, 4434, 1960, 3049, 3587, 54, 2722, 2259, 808, 1285, 3714, 2519, 3877, 3727, 831, 4408, 1867, 2757, 3297, 466, 526, 1039, 3421, 3875, 3092, 186, 2323, 1133, 3913, 924, 2198, 1810, 141, 3513, 2063, 1096, 3853, 3606, 2135, 2506, 2715, 3926, 3768, 3683, 2652, 3267, 24, 1078, 711, 2735, 3425, 1469, 2732, 1877, 3032, 3106, 750, 3990, 4054, 404, 3346, 3761, 1366, 1537, 95, 1644, 3825, 3713, 3039, 3918, 2960, 46, 507, 1840, 1254, 465, 2583, 843, 1544, 1718, 3708, 1788, 1032, 3035, 2078, 3390, 2849, 2372, 3118, 1409, 4486, 1789, 4096, 4450, 2908, 2005, 4260, 1105, 2741, 252, 2672, 1327, 3195, 765, 374, 4483, 2276, 2489, 2752, 1802, 2080, 2883, 1613, 4494, 1047, 2717, 66, 2036, 1946, 3974, 4279, 3157, 1701, 558, 2889, 1042, 2286, 4095, 2614, 1106, 1043, 4203, 899, 1422, 2221, 3995, 93, 3730, 3845, 798, 2241, 439, 487, 1694, 2134, 3407, 1306, 763, 348, 4493, 4277, 3757, 2112, 1031, 1823, 2925, 49, 1353, 3695, 2312, 3858, 2438, 255, 4275, 4094, 1266, 920, 3207, 4027, 4002, 2654, 1454, 3313, 2354, 2570, 4417, 623, 3048, 2980, 1831, 4394, 3752, 2608, 2599, 605, 1084, 2635, 3832, 3436, 2688, 19, 3194, 2113, 1969, 4214, 1891, 3856, 353, 2228, 3209, 436, 749, 3394, 3579, 571, 3453, 3363, 516, 349, 1123, 3245, 276, 742, 695, 337, 385, 3624, 2850, 2831, 3219, 3334, 559, 1112, 2765, 1480, 78, 68, 4134, 2148, 3609, 1011, 3605, 1272, 3641, 2759, 3873, 3667, 2061, 1238, 4160, 706, 2742, 2772, 2449, 311, 4053, 2484, 1137, 525, 201, 2477, 3314, 4121, 3107, 3797, 3958, 2041, 3986, 3105, 3359, 366, 2475, 1442, 2663, 4016, 696, 1668, 575, 1523, 836, 490, 1984, 1453, 373, 477, 498, 414, 1742, 2454, 391, 3638, 2427, 4049, 3604, 2361, 4152, 249, 421, 3891, 2846, 2129, 3852, 3128, 1309, 1163, 3722, 1688, 2812, 1862, 734, 3351, 1527, 4385, 1725, 1434, 1558, 1986, 2473, 519, 3174, 3619, 1809, 64, 2558, 2014, 3142, 2659, 2771, 704, 1209, 375, 4028, 3037, 2423, 1667, 2756, 1059, 1221, 2196, 3373, 3703, 459, 1086, 1761, 3972, 4038, 634, 1998, 2289, 1955, 2739, 936, 535, 1924, 2311, 2169, 4101, 1103, 1815, 3652, 146, 1191, 3149, 126, 553, 1594, 2826, 3538, 1821, 445, 512, 2963, 3459, 613, 1473, 1884, 1641, 2210, 2987, 3382, 3862, 1435, 3395, 4323, 1147, 318, 1676, 2600, 1617, 2993, 1965, 999, 1645, 3767, 1115, 1095, 1991, 1853, 2553, 2810, 383, 1692, 3814, 3255, 569, 3854, 4242, 2984, 1686, 1417, 3882, 1601, 1591, 933, 2967, 3716, 982, 3011, 2789, 1747, 4281, 1652, 1128, 963, 3050, 3735, 2442, 3496, 326, 3323, 3707, 166, 2087, 3134, 1136, 4255, 697, 2661, 4282, 2443, 2946, 702, 4025, 1942, 1901, 1609, 2675, 3556, 1486, 4159, 615, 862, 813, 163, 2268, 355, 1870, 3055, 3776, 1680, 16, 1744, 3277, 4089, 2437, 541, 2676, 1958, 1394, 3472, 1214, 1767, 2453, 238, 396, 3525, 457, 2412, 1858, 1542, 3129, 2040, 3422, 2065, 1716, 2895, 428, 2279, 1125, 4227, 3720, 3116, 1045, 1864, 1070, 3565, 2165, 3307, 1842, 2856, 303, 3404, 209, 3617, 170, 4442, 2429, 4496, 3805, 3383, 2592, 2914, 4155, 203, 3348, 4325, 2140, 2568, 3803, 1529, 753, 1839, 1525, 2121, 2015, 60, 4117, 2955, 1818, 3772, 482, 1223, 4327, 2176, 2128, 2451, 1187, 3739, 577, 3026, 4067, 2167, 1003, 1405, 3737, 136, 2515, 3630, 1377, 2774, 1973, 3632, 2805, 3199, 1463, 4212, 336, 606, 618, 235, 890, 2117, 1273, 4208, 2611, 530, 2300, 3800, 3848, 112, 1122, 2291, 4173, 2409, 1307, 3555, 216, 2283, 612, 2513, 2919, 2127, 1051, 3179, 2052, 4112, 3204, 3690, 3471, 350, 2253, 1944, 4446, 3900, 2544, 3196, 3080, 3668, 2582, 58, 4360, 3687, 528, 1484, 208, 4267, 1872, 248, 1421, 2696, 761, 3231, 3559, 2634, 2435, 2433, 1772, 1419, 3823, 2746, 3150, 2460, 2044, 3315, 2206, 4249, 4048, 1401, 1654, 2740, 135, 1790, 4463, 1313, 1275, 1151, 1166, 1536, 1562, 1555, 4314, 2201, 1952, 2422, 1035, 164, 4300, 2481, 4154, 2897, 1748, 2413, 2250, 2030, 2520, 153, 1962, 4438, 3759, 180, 2272, 2227, 3059, 1334, 2800, 2929, 549, 2101, 2879, 3371, 1918, 3807, 2461, 475, 2075, 1695, 3309, 3030, 2466, 2179, 935, 4403, 4490, 4056, 4161, 2839, 909, 4415, 1947, 1438, 4317, 889, 1424, 4252, 1398, 2816, 4363, 1050, 2370, 2076, 1977, 2355, 3879, 3508, 887, 2401, 2390, 1926, 206, 1439, 1210, 939, 3957, 975, 2930, 4156, 1607, 2181, 1804, 4128, 3711, 2602, 1487, 1880, 3226, 1259, 4332, 672, 4245, 2171, 1177, 2111, 3919, 2646, 2222, 3427, 738, 2277, 3263, 3576, 3052, 205, 4499, 506, 2107, 822, 674, 926, 2269, 1532, 3650, 3304, 1653, 131, 2434, 2629, 547, 2500, 2719, 1189, 2098, 378, 744, 3758, 2841, 17, 179, 1228, 2357, 3616, 2495, 1471, 175, 2406, 2780, 810, 1817, 2273, 4299, 2952, 1633, 3843, 2817, 3120, 731, 888, 188, 3570, 315, 2220, 1156, 2029, 2644, 307, 2905, 1498, 3651, 712, 2009, 610, 2938, 3969, 2939, 2064, 1030, 1721, 23, 1069, 125, 1326, 3747, 272, 3952, 312, 1340, 449, 3541, 3584, 4076, 1948, 1963, 844, 1602, 2827, 3175, 3088, 2887, 4199, 3111, 1882, 2682, 456, 2358, 165, 284, 3287, 2224, 3172, 3447, 143, 1153, 3417, 2728, 426, 602, 2109, 1776, 3091, 3473, 124, 4086, 1996, 4398, 1524, 140, 2787, 2562, 2194, 98, 3736, 1108, 4217, 3384, 2994, 3190, 1897, 2242, 3302, 1980, 3591, 2011, 4233, 1149, 3437, 154, 3841, 3717, 2848, 2304, 2010, 4457, 792, 3482, 354, 599, 4369, 767, 997, 684, 3221, 356, 780, 3326, 4043, 3479, 795, 1651, 1335, 319, 192, 4006, 3010, 3715, 4072, 2299, 444, 2235, 4257, 492, 1402, 2915, 1514, 683, 147, 2317, 1475, 3038, 2699, 991, 3679, 1656, 340, 2920, 2844, 3554, 4466, 779, 2534, 346, 3270, 1134, 2396, 732, 3675, 3721, 1183, 1925, 1923, 2420, 1215, 169, 4014, 3691, 838, 90, 2114, 2607, 707, 2432, 2679, 703, 392, 1934, 407, 297, 2569, 344, 3921, 664, 1168, 3380, 258, 243, 4423, 1337, 3312, 1002, 2062, 4492, 1494, 4125, 1869, 3311, 626, 4261, 22, 781, 4395, 2351, 3795, 145, 3498, 56, 4097, 1749, 1107, 1628, 3073, 63, 4453, 858, 1271, 4009, 677, 1930, 2940, 3186, 1578, 3826, 137, 1190, 11, 1143, 3369, 129, 2248, 83, 4356, 1201, 1298, 2966, 4328, 210, 240, 1548, 531, 3614, 678, 4209, 1100, 735, 214, 1807, 1345, 4491, 393, 1426, 3043, 4042, 2262, 2395, 840, 1087, 3045, 2472, 636, 4182, 1261, 4165, 332, 966, 257, 103, 2872, 1158, 676, 632, 3999, 722, 3773, 568, 1041, 251, 4372, 4143, 796, 3809, 4441, 3162, 2244, 2769, 3833, 338, 3176, 2035, 3167, 1860, 2072, 1800, 496, 292, 1648, 3535, 1816, 3788, 295, 3666, 2770, 2457, 2292, 1395, 680, 3325, 3581, 35, 451, 2556, 688, 432, 922, 196, 2050, 2591, 1226, 2156, 1956, 4176, 1482, 4228, 915, 102, 2597, 1580, 1711, 3934, 1080, 783, 1834, 3113, 1704, 70, 44, 334, 3096, 2598, 3441, 2835, 4294, 2541, 751, 3903, 3923, 1533, 3931, 2374, 2219, 4459, 1873, 2482, 1573, 1995, 1391, 1420, 1581, 4407, 1018, 2240, 2170, 3283, 4449, 3399, 2371, 2749, 2074, 1296, 1736, 1682, 4351, 2564, 2053, 3686, 3358, 2714, 458, 2818, 4296, 4118, 769, 2943, 4122, 245, 2298, 3103, 2424, 3961, 804, 2315, 3572, 395, 1117, 3476, 1857, 2596, 3844, 3168, 1642, 1988, 2985, 1685, 694, 3522, 627, 3056, 745, 513, 1782, 1693, 4410, 158, 2496, 517, 540, 633, 1211, 511, 4029, 1964, 1729, 3029, 10, 2625, 3402, 3561, 2947, 1444, 3855, 3953, 384, 1329, 3649, 1841, 3445, 4240, 219, 1608, 3927, 1771, 3627, 339, 1936, 2565, 803, 1917, 2653, 3784, 270, 3240, 1650, 4397, 1124, 2776, 4046, 394, 1280, 3420, 2748, 370, 893, 3798, 232, 3629, 1619, 376, 2803, 1506, 2188, 2811, 1759, 1563, 4106, 2402, 3090, 4153, 3322, 1299, 1048, 4185, 2441, 1465, 3242, 1672, 2681, 443, 1846, 1251, 646, 3412, 2595, 2150, 938, 3461, 34, 3387, 3271, 3890, 3878, 3430, 3074, 1199, 3203, 415, 1720, 1583, 2733, 1792, 4273, 3838, 3392, 4427, 625, 2525, 891, 3324, 1452, 3790, 4219, 2378, 3569, 4295, 2829, 3792, 1665, 15, 604, 1739, 2521, 236, 2214, 107, 3527, 194, 3449, 3994, 652, 628, 4145, 3222, 2894, 1118, 1359, 551, 4010, 1382, 660, 1312, 2271, 4130, 2857, 474, 1868, 2514, 4384, 3930, 3660, 57, 1675, 4365, 1825, 3086, 543, 3197, 2178, 3275, 4026, 1726, 841, 2405, 4222, 4238, 3577, 2601, 314, 773, 3996, 320, 1526, 4126, 322, 3027, 3137, 2858, 1414, 1175, 4144, 381, 1333, 2042, 4090, 3748, 1943, 4335, 2411, 4019, 1239, 4178, 666, 2673, 1557, 2238, 3465, 1596, 2213, 2182, 998, 1415, 2823, 1597, 1290, 1769, 28, 2, 1793, 72, 2904, 1216, 789, 1355, 1606, 2444, 945, 3909, 2007, 2073, 2761, 3968, 1346, 1689, 802, 157, 1585, 402, 229, 1323, 675, 501, 489, 1734, 2055, 324, 2694, 4239, 293, 3553, 2724, 4393, 845, 3256, 736, 2335, 2471, 3007, 3372, 3945, 3100, 455, 2037, 3396, 1854, 1090, 729, 1441, 480, 3017, 3450, 3053, 2648, 3567, 1811, 3894, 1830, 932, 788, 1599, 4387, 2301, 3731, 3819, 3289, 286, 2019, 3885, 3212, 2510, 624, 3259, 4302, 667, 2398, 4207, 3917, 106, 2316, 1094, 2205, 3487, 1252, 1049, 1206, 2537, 1845, 3139, 1713, 582, 1129, 4458, 2923, 1808, 2068, 3669, 2804, 71, 2084, 1044, 1592, 2319, 3110, 1850, 3698, 3785, 3063, 3892, 3783, 2380, 3019, 1161, 2778, 438, 3633, 2581, 2403, 1603, 3318, 2132, 1427, 3234, 1219, 1829, 2086, 3578, 1774, 3988, 3298, 3692, 4473, 2995, 288, 580, 3537, 3938, 3970, 36, 2760, 285, 509, 3349, 3501, 2902, 3915, 3248, 3356, 670, 3560, 3738, 424, 3912, 1160, 1200, 719, 1085, 3742, 1622, 3529, 1348, 3908, 2822, 799, 3431, 3295, 630, 1410, 4206, 2321, 3499, 3249, 3911, 1292, 3505, 41, 1220, 1899, 3688, 222, 748, 2191, 3388, 3719, 2392, 875, 4321, 4234, 1975, 2508, 2488, 805, 812, 523, 929, 609, 13, 1501, 4085, 2981, 4116, 1157, 2368, 3631, 825, 1385, 1182, 3802, 3766, 2656, 3524, 1188, 299, 1330, 2546, 2754, 413, 1354, 183, 2941, 3330, 1700, 237, 851, 2212, 1516, 2797, 3119, 3464, 454, 1186, 233, 4409, 578, 100, 1028, 3997, 1913, 4263, 978, 1659, 3280, 3794, 481, 4298, 4196, 4186, 4309, 4220, 121, 1881, 1008, 905, 1863, 2863, 2091, 567, 1061, 1155, 3104, 1875, 1054, 2003, 4124, 37, 3600, 2677, 1509, 2801, 3112, 3008, 2094, 3861, 253, 1344, 1371, 3451, 1812, 588, 3511, 700, 3424, 3514, 3801, 911, 2982, 2467, 563, 3622, 902, 3397, 2342, 3962, 1626, 3400, 1762, 3285, 3955, 3734, 2266, 460, 2837, 718, 1883, 3497, 2950, 2512, 447, 585, 2899, 2860, 3733, 4406, 2906, 2650, 3870, 1600, 400, 3644, 2428, 1315, 4110, 1343, 3125, 111, 1249, 1649, 230, 3895, 4087, 2431, 1966, 2684, 914, 2645, 1088, 3520, 1766, 4104, 25, 67, 4204, 2137, 2154, 3166, 1752, 2384, 2414, 3357, 2469, 877, 2796, 747, 3486, 3115, 4437, 3886, 1397, 2901, 3881, 4383, 2365, 3507, 1887, 3257, 1057, 971, 2478, 2022, 3959, 2297, 317, 2362, 269, 4108, 3851, 518, 3551, 3937, 3414, 3332, 369, 3353, 43, 1640, 2972, 1074, 242, 1722, 3950, 4017, 109, 3654, 1922, 2628, 96, 3066, 3002, 2234, 1369, 1904, 4424, 2386, 601, 723, 3626, 2006, 1895, 4338, 2971, 917, 987, 1803, 2535, 904, 469, 2999, 2572, 2229, 3225, 1717, 3883, 2288, 785, 2706, 3750, 42, 2377, 1611, 181, 4151, 3319, 3138, 4283, 2636, 2516, 2324, 3260, 2447, 1566, 811, 1796, 1058, 953, 826, 3793, 2621, 4266, 653, 994, 3131, 3984, 4478, 3787, 149, 167, 2777, 1150, 411, 3210, 405, 2233, 3623, 638, 52, 3564, 446, 287, 2785, 654, 2575, 3040, 0, 594, 4247, 1350, 2095, 642, 3517, 442, 202, 45, 2639, 2542, 4416, 1477, 2527, 1374, 3866, 4035, 529, 1281, 662, 4193, 2302, 220, 2561, 3223, 1062, 3251, 2034, 3076, 39, 4013, 589, 2492, 1092, 3672, 2017, 1357, 3975, 4480, 3544, 539, 3865, 2936, 3751, 4146, 1016, 3101, 1293, 2630, 3339, 371, 2190, 3329, 3438, 2802, 2138, 4349, 361, 2382, 1574, 3657, 1245, 2782, 2287, 4120, 3152, 1384, 390, 2587, 1360, 2186, 410, 2584, 2554, 1318, 1476, 461, 2173, 2322, 3642, 3079, 1709, 1708, 1508, 2494, 980, 2937, 3897, 3811, 3457, 4015, 665, 1040, 4008, 4337, 416, 3863, 2795, 1727, 3656, 1702, 1436, 3081, 2808, 3132, 2285, 833, 2081, 191, 3490, 142, 1225, 2480, 856, 3355, 1241, 1620, 1455, 3241, 4052, 629, 412, 2876, 546, 1568, 882, 2768, 1586, 591, 709, 1584, 1339, 4021, 2024, 2716, 2687, 1994, 2713, 2949, 947, 1294, 486, 441, 4374, 3684, 2618, 699, 1159, 2678, 246, 3087, 3268, 1308, 3341, 476, 478, 1824, 737, 4177, 2426, 757, 1781, 3067, 685, 4018, 2352, 981, 4109, 87, 3165, 1616, 1193, 2033, 342, 296, 1916, 607, 876, 1265, 3163, 4004, 2125, 1170, 4430, 2281, 1347, 464, 4235]/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/usr/local/lib/python3.9/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "

unlab_idxs: [1614, 4454, 298, 3647, 2226, 1104, 1472, 1546, 1324, 1549, 2566, 1464, 4211, 3491, 2965, 132, 587, 4040, 2638, 928, 3689, 3887, 4289, 1595, 2821, 557, 3504, 1784, 2968, 3571, 260, 2376, 4236, 3820, 3944, 3840, 4226, 365, 1590, 2992, 357, 31, 3135, 3378, 1979, 1552, 3415, 4404, 440, 4065, 1485, 3182, 3847, 983, 1731, 2517, 3333, 1235, 4194, 178, 1174, 3114, 277, 1730, 3083, 1328, 3375, 527, 2703, 1740, 3645, 2397, 2799, 3238, 264, 4462, 3914, 3206, 2631, 1915, 3278, 1064, 4011, 908, 2528, 2551, 467, 494, 3291, 2139, 923, 3082, 2988, 1691, 2490, 2155, 4201, 2143, 4272, 1937, 4111, 1479, 2997, 973, 2900, 2421, 1278, 1572, 3403, 155, 1305, 2711, 4132, 3536, 4221, 3744, 1113, 1356, 1025, 1425, 4472, 2387, 3075, 1889, 3740, 4171, 1647, 2243, 3681, 306, 4061, 3837, 1321, 2861, 2668, 1233, 3815, 1945, 2172, 2336, 1372, 274, 2366, 26, 271, 2353, 2791, 883, 2265, 979, 174, 3419, 2664, 104, 839, 2729, 716, 3593, 864, 878, 2503, 134, 3374, 4024, 195, 3320, 1456, 3860, 2708, 3607, 3483, 12, 1367, 521, 2012, 4412, 4058, 993, 3964, 1756, 1909, 4380, 3563, 4400, 3625, 639, 3288, 3350, 3217, 2115, 3725, 2031, 1164, 2136, 3960, 3621, 2830, 1430, 631, 4084, 3599, 2933, 139, 4470, 2531, 4340, 3335, 3989, 3528, 885, 1071, 1859, 3762, 2834, 520, 2626, 3786, 3235, 562, 645, 4389, 2327, 2130, 397, 1560, 4139, 4324, 4020, 256, 3133, 2845, 2580, 2505, 2043, 108, 3674, 3154, 2932, 3746, 2385, 1515, 1361, 73, 4401, 2911, 960, 656, 3057, 2246, 794, 61, 2924, 367, 4345, 2256, 972, 1657, 3477, 1512, 4102, 3729, 247, 595, 3122, 3932, 2341, 1961, 4218, 1207, 2202, 3615, 2921, 3512, 3871, 2200, 766, 1458, 3956, 372, 1270, 4055, 4103, 931, 721, 1785, 2468, 2885, 1006, 3539, 1490, 1470, 79, 2197, 3948, 3896, 1779, 1203, 897, 3054, 4444, 3006, 1970, 2762, 4064, 3, 3274, 3592, 1950, 2669, 1213, 4129, 3954, 225, 2613, 2609, 2446, 2507, 3236, 2573, 2792, 1559, 3379, 4113, 717, 2881, 2274, 2539, 4291, 1342, 3905, 262, 4330, 1297, 1332, 1229, 3493, 4032, 2662, 368, 1905, 1760, 3198, 3310, 2892, 961, 433, 228, 4187, 1511, 985, 2536, 1587, 1322, 4320, 3515, 3836, 1575, 4175, 4119, 4092, 2047, 4259, 3595, 2223, 1413, 3928, 586, 4270, 3636, 4382, 1732, 3701, 3817, 1797, 1024, 113, 351, 755, 3835, 3261, 2589, 2660, 1914, 2916, 1171, 2058, 3158, 2788, 4370, 3665, 669, 1121, 2264, 3648, 3170, 1780, 1643, 3156, 1940, 3433, 123, 1510, 2865, 448, 3033, 743, 2305, 4174, 3462, 3562, 2103, 3916, 2070, 2393, 3755, 3220, 1258, 1468, 4276, 786, 2622, 532, 3754, 3694, 2157, 3406, 1388, 4078, 4205, 1577, 2049, 640, 1077, 3089, 2557, 1026, 873, 2969, 3523, 4297, 4348, 3888, 2953, 3580, 1000, 1939, 3004, 995, 759, 1370, 4440, 4083, 2425, 4475, 1674, 760, 1034, 834, 187, 2295, 570, 1624, 1060, 1423, 879, 2806, 1935, 3391, 2868, 2175, 4443, 1535, 1786, 583, 4059, 1902, 2470, 3164, 910, 1551, 1489, 160, 3611, 2458, 3093, 4, 1093, 82, 2067, 4066, 3021, 4202, 2284, 1169, 161, 2913, 828, 2038, 870, 2501, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999]
Pseudo-Label-v1 2013 with iteration pseudo labels
------ Training epochs: 0 ------
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
[train][0  ] lloss: 2.41964	uloss: N/A	lacc: 7.031%	uacc: N/A
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
>>>[train] lloss: 28.00306	uloss: N/A	lacc: 34.323%	uacc: N/A
------ Testing epochs: 0 ------
[test][0  ] lloss: 2.76893	lacc: 21.094%
>>>[test] lloss: 21.13312	lacc: 20.500%
------ Training epochs: 1 ------
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
[train][0  ] lloss: 1.61383	uloss: N/A	lacc: 39.062%	uacc: N/A
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
>>>[train] lloss: 24.07225	uloss: N/A	lacc: 40.833%	uacc: N/A
------ Testing epochs: 1 ------
[test][0  ] lloss: 1.78894	lacc: 32.031%
>>>[test] lloss: 15.08900	lacc: 31.500%
------ Training epochs: 2 ------
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
[train][0  ] lloss: 1.43335	uloss: N/A	lacc: 42.969%	uacc: N/A
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
>>>[train] lloss: 22.24519	uloss: N/A	lacc: 44.948%	uacc: N/A
------ Testing epochs: 2 ------
[test][0  ] lloss: 1.82144	lacc: 32.812%
>>>[test] lloss: 14.69653	lacc: 30.500%
------ Training epochs: 3 ------
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
[train][0  ] lloss: 1.43592	uloss: N/A	lacc: 46.094%	uacc: N/A
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
>>>[train] lloss: 20.57912	uloss: N/A	lacc: 49.635%	uacc: N/A
------ Testing epochs: 3 ------
[test][0  ] lloss: 1.70239	lacc: 37.500%
>>>[test] lloss: 14.12043	lacc: 33.400%
------ Training epochs: 4 ------
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
[train][0  ] lloss: 1.36906	uloss: N/A	lacc: 50.000%	uacc: N/A
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
outputs[umask]:tensor([], device='cuda:0', size=(0, 10), grad_fn=<IndexBackward0>)
iter_unlab_pslab[umask]:tensor([], device='cuda:0', dtype=torch.int64)
tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)
>>>[train] lloss: 20.12823	uloss: N/A	lacc: 50.781%	uacc: N/A
------ Testing epochs: 4 ------
[test][0  ] lloss: 1.78186	lacc: 32.031%
>>>[test] lloss: 14.42548	lacc: 31.100%
------ Training epochs: 5 ------
